{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f2d062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c2c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"./test-data/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a12276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_missing_data(df, missing_rate, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df_missing = df.copy()\n",
    "    mask = rng.random(len(df_missing)) < missing_rate\n",
    "    df_missing.loc[mask, \"throughput_bps\"] = np.nan\n",
    "    print(f\"Introduced {missing_rate * 100}% missing data.\")\n",
    "    # print(f\"Mask: {mask}\")\n",
    "    return df_missing\n",
    "\n",
    "datasets_missing = {}\n",
    "\n",
    "for file in os.listdir(BASE_DIR):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(BASE_DIR, file))\n",
    "        base_key = file.removesuffix(\".throughput.csv\")  \n",
    "        datasets_missing[base_key] = {} \n",
    "        \n",
    "        for rate in [0.1, 0.2, 0.3, 0.4]:\n",
    "            df_missing = introduce_missing_data(df, missing_rate=rate, seed=42)\n",
    "            rate_key = f\"{int(rate * 100)}\"\n",
    "            datasets_missing[base_key][rate_key] = df_missing\n",
    "\n",
    "datasets_missing  # DataFrame com 10% de dados faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae4b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a6ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94b073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_knn_imputer(df_missing, k=5):\n",
    "    df_imp = df_missing.copy()\n",
    "\n",
    "    imputer = KNNImputer(n_neighbors=k, weights=\"uniform\")\n",
    "    imputed_values = imputer.fit_transform(df_imp[[\"throughput_bps\"]])\n",
    "\n",
    "    df_imp[\"throughput_bps\"] = imputed_values[:, 0]\n",
    "    return df_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3d34bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.statespace.structural import UnobservedComponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b9d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_kalman(\n",
    "    df_missing,\n",
    "    model=\"arima\",\n",
    "    # --- ARIMA/SARIMA params ---\n",
    "    arima_order=(1, 1, 1),\n",
    "    seasonal_order=(0, 0, 0, 0),\n",
    "    # --- Estrutural params ---\n",
    "    level=\"local level\",         # opções comuns: \"local level\", \"local linear trend\"\n",
    "    seasonal_period=None         # por ex., 24 p/ sazonalidade diária em dados horários\n",
    "):\n",
    "    \"\"\"\n",
    "    Imputa NaNs em 'throughput_bps' via Kalman smoothing.\n",
    "\n",
    "    Parâmetros\n",
    "    ----------\n",
    "    df_missing : pd.DataFrame\n",
    "        DataFrame com colunas 'time' (datetime string com timezone) e 'throughput_bps'.\n",
    "    model : {'arima', 'structural'}\n",
    "        Escolhe a abordagem:\n",
    "        - 'arima': ajusta SARIMAX(ARIMA/SARIMA) e usa as previsões in-sample (Kalman).\n",
    "        - 'structural': ajusta UnobservedComponents (nível/trend/seasonal) + Kalman.\n",
    "    arima_order : tuple\n",
    "        Ordem (p, d, q) do ARIMA.\n",
    "    seasonal_order : tuple\n",
    "        Ordem sazonal (P, D, Q, s) para SARIMA.\n",
    "    level : str\n",
    "        Componente de nível do modelo estrutural (ex.: 'local level', 'local linear trend').\n",
    "    seasonal_period : int or None\n",
    "        Período sazonal para o modelo estrutural (ex.: 24, 7*24, etc.). None = sem sazonalidade.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    df_imputed : pd.DataFrame\n",
    "        Cópia de df_missing com 'throughput_bps' imputado nos pontos NaN.\n",
    "    \"\"\"\n",
    "    df_imp = df_missing.copy()\n",
    "\n",
    "    # Garante dtype datetime (assumido correto e com tz, sem tratamento de erros)\n",
    "    t = pd.to_datetime(df_imp[\"time\"])\n",
    "    y = df_imp[\"throughput_bps\"].astype(float)\n",
    "\n",
    "    # Máscara de faltantes\n",
    "    miss_mask = y.isna()\n",
    "\n",
    "    if model == \"arima\":\n",
    "        # SARIMAX lida com NaNs na endógena e usa Kalman internamente\n",
    "        mod = SARIMAX(\n",
    "            y,\n",
    "            order=arima_order,\n",
    "            seasonal_order=seasonal_order,\n",
    "            enforce_stationarity=False,\n",
    "            enforce_invertibility=False\n",
    "        )\n",
    "        res = mod.fit(disp=False)\n",
    "\n",
    "        # Previsão in-sample (predicted_mean) já incorpora filtro/smoother de Kalman\n",
    "        y_hat = res.get_prediction().predicted_mean\n",
    "\n",
    "    elif model == \"structural\":\n",
    "        # Modelo estrutural: nível / tendência / sazonal (state-space) + Kalman\n",
    "        # Ex.: level='local level' ou 'local linear trend'\n",
    "        #     seasonal_period define sazonalidade (ex.: 24 p/ hora, 7*24 p/ semanal, etc.)\n",
    "        ucm = UnobservedComponents(\n",
    "            y,\n",
    "            level=level,\n",
    "            seasonal=seasonal_period  # None => sem sazonal\n",
    "        )\n",
    "        res = ucm.fit(disp=False)\n",
    "\n",
    "        # predicted_mean é a série observável estimada (alisada) pelo modelo\n",
    "        y_hat = res.get_prediction().predicted_mean\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"model must be 'arima' or 'structural'\")\n",
    "\n",
    "    # Imputa apenas onde havia NaN, preservando os valores observados\n",
    "    y_imp = y.copy()\n",
    "    y_imp[miss_mask] = y_hat[miss_mask].values\n",
    "\n",
    "    df_imp[\"throughput_bps\"] = y_imp.values\n",
    "    return df_imp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b9a052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Moving Average (SMA) centrada ---\n",
    "def impute_moving_average(df_missing, window=5, center=True):\n",
    "    \"\"\"\n",
    "    Imputa NaNs em 'throughput_bps' usando média móvel (SMA).\n",
    "    \n",
    "    Ideia:\n",
    "      - Calcula a média dos vizinhos dentro de uma janela.\n",
    "      - Aqui usamos janela centrada (anterior e posterior) e min_periods=1\n",
    "        para sempre produzir valor mesmo com poucas observações na borda.\n",
    "      - Substitui apenas os pontos que eram NaN originalmente.\n",
    "\n",
    "    Parâmetros\n",
    "      window: tamanho da janela (3–10 usualmente)\n",
    "      center: True para janela centrada (vizinho passado e futuro)\n",
    "    \"\"\"\n",
    "    df_imp = df_missing.copy()\n",
    "    y = df_imp[\"throughput_bps\"]\n",
    "\n",
    "    # Média móvel centrada; ignora NaNs automaticamente; min_periods=1 para não “perder” bordas\n",
    "    sma = y.rolling(window=window, center=center, min_periods=1).mean()\n",
    "\n",
    "    mask_missing = y.isna()\n",
    "    df_imp.loc[mask_missing, \"throughput_bps\"] = sma[mask_missing].values\n",
    "    return df_imp\n",
    "\n",
    "\n",
    "# --- Exponentially Weighted Moving Average (EWMA) ---\n",
    "def impute_ewma(df_missing, alpha=0.2):\n",
    "    \"\"\"\n",
    "    Imputa NaNs em 'throughput_bps' usando EWMA (média móvel exponencial).\n",
    "    \n",
    "    Ideia:\n",
    "      - EWMA pondera mais os valores recentes (decaimento controlado por alpha).\n",
    "      - Aqui usamos a forma “causal” (somente passado) com adjust=False (recursivo padrão).\n",
    "      - Substitui apenas os pontos que eram NaN originalmente.\n",
    "\n",
    "    Parâmetros\n",
    "      alpha: 0.1–0.3 é um intervalo comum; quanto maior, mais peso ao mais recente.\n",
    "    \"\"\"\n",
    "    df_imp = df_missing.copy()\n",
    "    y = df_imp[\"throughput_bps\"]\n",
    "\n",
    "    ew = y.ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "    mask_missing = y.isna()\n",
    "    df_imp.loc[mask_missing, \"throughput_bps\"] = ew[mask_missing].values\n",
    "    return df_imp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# ----------------------------\n",
    "# 1) PERIOD (n_lines) ESTIMATION\n",
    "# ----------------------------\n",
    "\n",
    "def _acf(y: np.ndarray, max_lag: int) -> np.ndarray: # ->basicamente isso aqui calcula a galera com maior correlação e deixa em uma só linha\n",
    "    \"\"\"Biased ACF up to max_lag (lag 0..max_lag). NaNs are linearly interpolated first.\"\"\"\n",
    "    y = pd.Series(y).interpolate(limit_direction=\"both\").to_numpy()\n",
    "    y = y - np.nanmean(y)\n",
    "    n = len(y)\n",
    "    acf_vals = np.empty(max_lag + 1)\n",
    "    denom = np.dot(y, y) + 1e-12\n",
    "    for lag in range(max_lag + 1):\n",
    "        acf_vals[lag] = np.dot(y[: n - lag], y[lag:]) / denom\n",
    "    return acf_vals\n",
    "\n",
    "def _fft_period(y: np.ndarray, min_period: int, max_period: int) -> Optional[int]: # isso aqui faz a mesma coisa só que com fourrier \n",
    "    \"\"\"FFT-based dominant period in [min_period, max_period], None if not found.\"\"\"\n",
    "    y = pd.Series(y).interpolate(limit_direction=\"both\").to_numpy()\n",
    "    y = y - np.mean(y)\n",
    "    n = len(y)\n",
    "    if n < 4:\n",
    "        return None\n",
    "    # Real FFT spectrum\n",
    "    spec = np.fft.rfft(y)\n",
    "    freqs = np.fft.rfftfreq(n, d=1.0)  # assume unit sampling\n",
    "    # Exclude DC\n",
    "    mask = freqs > 0\n",
    "    freqs = freqs[mask]\n",
    "    power = (spec[mask].real**2 + spec[mask].imag**2)\n",
    "    # Convert freq -> period\n",
    "    periods = np.round(1.0 / freqs).astype(int)\n",
    "    # Keep only within bounds\n",
    "    sel = (periods >= min_period) & (periods <= max_period)\n",
    "    if not np.any(sel):\n",
    "        return None\n",
    "    # Aggregate power by period (many freqs can map to same rounded period)\n",
    "    dfp = pd.DataFrame({\"period\": periods[sel], \"power\": power[sel]})\n",
    "    top = dfp.groupby(\"period\", as_index=False)[\"power\"].sum().sort_values(\"power\", ascending=False)\n",
    "    return int(top[\"period\"].iloc[0]) if len(top) else None\n",
    "\n",
    "def estimate_period(\n",
    "    y: np.ndarray,\n",
    "    min_period: int = 4,\n",
    "    max_period: Optional[int] = None\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Pick period (n_lines) automatically using ACF peak with FFT fallback.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    n = len(y)\n",
    "    if max_period is None:\n",
    "        max_period = max(7, min(n // 4, 1000))  # sensible cap\n",
    "\n",
    "    if n < min_period * 2:\n",
    "        # too short — just return something small\n",
    "        return max(min_period, min(n, 8))\n",
    "\n",
    "    acf_vals = _acf(y, max_period)\n",
    "    # Ignore lag 0; pick the best lag in [min_period, max_period]\n",
    "    candidate_lags = np.arange(min_period, max_period + 1)\n",
    "    best_lag = candidate_lags[np.argmax(acf_vals[min_period: max_period + 1])]\n",
    "\n",
    "    # FFT fallback check: if ACF peak is weak, try FFT suggestion\n",
    "    acf_strength = acf_vals[best_lag]\n",
    "    fft_suggestion = _fft_period(y, min_period, max_period)\n",
    "    if fft_suggestion is not None:\n",
    "        if acf_strength < 0.15:  # weak ACF; trust FFT\n",
    "            return int(fft_suggestion)\n",
    "        # If both agree closely, prefer the smaller (more stable) period\n",
    "        if abs(fft_suggestion - best_lag) <= 2:\n",
    "            return int(min(fft_suggestion, best_lag))\n",
    "    return int(best_lag)\n",
    "\n",
    "# ---------------------------------\n",
    "# 2) FOLDING (PERIODIC TEMPORAL MATRIX)\n",
    "# ---------------------------------\n",
    "\n",
    "def fold_series_to_matrix(y: np.ndarray, period: int) -> Tuple[np.ndarray, int]: # basicamente isso cria a matriz temporal \n",
    "    \"\"\"\n",
    "    Fold 1D series into a (n_blocks, period) matrix. \n",
    "    Pads the last block with NaN if needed.\n",
    "    Returns (M, original_len).\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    n = len(y)\n",
    "    n_blocks = int(np.ceil(n / period)) #n de coluans\n",
    "    pad_len = n_blocks * period - n\n",
    "    if pad_len > 0:\n",
    "        y = np.concatenate([y, np.full(pad_len, np.nan)])\n",
    "    M = y.reshape(n_blocks, period)\n",
    "    return M, n\n",
    "\n",
    "def unfold_matrix_to_series(M: np.ndarray, original_len: int) -> np.ndarray: # volta pro formato original\n",
    "    \"\"\"Inverse of fold: row-wise flatten and trim to original length.\"\"\"\n",
    "    y = M.reshape(-1)\n",
    "    return y[:original_len]\n",
    "\n",
    "# ----------------------------\n",
    "# 3) SVD + RANK SELECTION\n",
    "# ----------------------------\n",
    "\n",
    "def svd_rank(M_filled: np.ndarray, energy: float = 0.9) -> Tuple[np.ndarray, np.ndarray, np.ndarray, int]: # faz o svd na tora e escolhe o r com a soma dos valore sisnuglares so deus sabe pq\n",
    "    \"\"\"\n",
    "    Compute SVD and choose rank r by cumulative explained 'energy' (sum of singular values).\n",
    "    \"\"\"\n",
    "    U, s, Vt = np.linalg.svd(M_filled, full_matrices=False)\n",
    "    cum = np.cumsum(s) / (np.sum(s) + 1e-12)\n",
    "    r = int(np.searchsorted(cum, energy) + 1)\n",
    "    r = max(1, min(r, min(M_filled.shape)))\n",
    "    return U, s, Vt, r\n",
    "\n",
    "# -------------------------------------------\n",
    "# 4) KNN IMPUTE USING ROW EMBEDDINGS FROM SVD\n",
    "# -------------------------------------------\n",
    "\n",
    "def _warm_start_fill(M: np.ndarray) -> np.ndarray: # coloca mediana em tudo e vapo só pra começar\n",
    "    \"\"\"Column-wise median fill as a stable warm start.\"\"\"\n",
    "    M_filled = M.copy()\n",
    "    col_medians = np.nanmedian(M_filled, axis=0)\n",
    "    # If an entire column is NaN, fallback to global median\n",
    "    if np.any(np.isnan(col_medians)):\n",
    "        global_med = np.nanmedian(M_filled)\n",
    "        col_medians = np.where(np.isnan(col_medians), global_med, col_medians)\n",
    "    inds = np.where(np.isnan(M_filled))\n",
    "    M_filled[inds] = np.take(col_medians, inds[1])\n",
    "    return M_filled\n",
    "\n",
    "def impute_with_knn_in_latent( \n",
    "    M: np.ndarray,\n",
    "    k: int = 5,\n",
    "    energy: float = 0.9,\n",
    "    allow_future: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Impute NaNs in M by KNN in SVD latent space (rows ≈ cycles/weeks). que porra eh latent space\n",
    "    For each missing cell (i,j), find k nearest rows to row i in latent space\n",
    "    among those with M[row, j] observed (and optionally row < i).\n",
    "    \"\"\"\n",
    "    M_filled0 = _warm_start_fill(M)\n",
    "    U, s, Vt, r = svd_rank(M_filled0, energy=energy)\n",
    "    # Row embeddings (T x r): U_r * S_r\n",
    "    Z = U[:, :r] * s[:r]  # broadcasting: each column of U scaled by s\n",
    "\n",
    "    M_imp = M.copy()\n",
    "    T, P = M.shape\n",
    "    eps = 1e-8\n",
    "\n",
    "    # Precompute which rows have each column observed\n",
    "    observed_mask = ~np.isnan(M)\n",
    "    for i in range(T):\n",
    "        # indices (columns) that are missing in row i\n",
    "        miss_cols = np.where(~observed_mask[i])[0]\n",
    "        if len(miss_cols) == 0:\n",
    "            continue\n",
    "        zi = Z[i]\n",
    "\n",
    "        # Candidate rows for neighbors (global, filtered below per col)\n",
    "        if allow_future:\n",
    "            candidate_rows_global = np.arange(T)\n",
    "        else:\n",
    "            candidate_rows_global = np.arange(0, i)  # only past\n",
    "\n",
    "        if len(candidate_rows_global) == 0:\n",
    "            # If we can't use past (i==0), allow future just for this row:\n",
    "            candidate_rows_global = np.arange(T)\n",
    "\n",
    "        # Distances in latent space to all candidates\n",
    "        Zc = Z[candidate_rows_global]\n",
    "        dists = np.linalg.norm(Zc - zi[None, :], axis=1)\n",
    "        dists = dists + eps  # avoid zero\n",
    "\n",
    "        for j in miss_cols:\n",
    "            # keep only candidates that have this column observed\n",
    "            obs_rows = candidate_rows_global[observed_mask[candidate_rows_global, j]]\n",
    "            if len(obs_rows) == 0:\n",
    "                # fall back to warm-start value if nothing observed\n",
    "                M_imp[i, j] = M_filled0[i, j]\n",
    "                continue\n",
    "\n",
    "            # distances for those rows\n",
    "            d = np.linalg.norm(Z[obs_rows] - zi[None, :], axis=1) + eps\n",
    "            # k nearest\n",
    "            if len(d) > k:\n",
    "                idx = np.argpartition(d, k)[:k]\n",
    "                nn_rows = obs_rows[idx]\n",
    "                d = d[idx]\n",
    "            else:\n",
    "                nn_rows = obs_rows\n",
    "\n",
    "            w = 1.0 / d  # inverse-distance weights\n",
    "            vals = M[nn_rows, j]\n",
    "            # safety: if still NaN (shouldn't happen), drop them\n",
    "            ok = ~np.isnan(vals)\n",
    "            if not np.any(ok):\n",
    "                M_imp[i, j] = M_filled0[i, j]\n",
    "            else:\n",
    "                vals = vals[ok]\n",
    "                w = w[ok]\n",
    "                M_imp[i, j] = np.sum(w * vals) / np.sum(w)\n",
    "\n",
    "    return M_imp\n",
    "\n",
    "# ----------------------------\n",
    "# 5) MAIN PIPELINE\n",
    "# ----------------------------\n",
    "\n",
    "def impute_throughput_svd_knn(\n",
    "    df: pd.DataFrame,\n",
    "    col: str = \"throughput_bps\",\n",
    "    min_period: int = 4,\n",
    "    max_period: Optional[int] = None,\n",
    "    energy: float = 0.9,\n",
    "    k: int = 5,\n",
    "    allow_future: bool = True\n",
    ") -> Tuple[pd.Series, dict]:\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "    1) auto period detection -> n_lines (period)\n",
    "    2) fold into (n_blocks, period) matrix\n",
    "    3) SVD -> latent row embeddings\n",
    "    4) KNN in latent space to impute NaNs\n",
    "    5) unfold back to 1D series\n",
    "    Returns (imputed_series, diagnostics).\n",
    "    \"\"\"\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column '{col}' not found in df.\")\n",
    "\n",
    "    y = df[col].to_numpy(dtype=float)\n",
    "    original_index = df.index\n",
    "\n",
    "    # Detect period (n_lines)\n",
    "    period = estimate_period(y, min_period=min_period, max_period=max_period)\n",
    "\n",
    "    # Fold\n",
    "    M, orig_len = fold_series_to_matrix(y, period=period)\n",
    "\n",
    "    # Impute in latent KNN space\n",
    "    M_imp = impute_with_knn_in_latent(M, k=k, energy=energy, allow_future=allow_future)\n",
    "\n",
    "    # Unfold\n",
    "    y_imp = unfold_matrix_to_series(M_imp, original_len=orig_len)\n",
    "\n",
    "    print(y_imp)\n",
    "\n",
    "    diagnostics = {\n",
    "        \"period_estimated\": period,\n",
    "        \"matrix_shape\": M.shape,\n",
    "        \"rank_energy_target\": energy,\n",
    "        \"k_neighbors\": k,\n",
    "        \"allow_future\": allow_future,\n",
    "    }\n",
    "    return pd.Series(y_imp, index=original_index, name=f\"{col}_imputed\"), diagnostics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af88b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from fancyimpute import SoftImpute, IterativeSVD\n",
    "# from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# def impute_hankel_knn_svd(\n",
    "#     df_missing,\n",
    "#     window=168,          # tamanho da janela L da Hankel (p.ex. ~1/2 a 1x do período sazonal)\n",
    "#     k=5,                # vizinhos do KNN time-indexed\n",
    "#     blend=0.2,          # mistura final entre SVD e KNN nos pontos faltantes (0 = só SVD)\n",
    "#     seasonal_period=None  # None => inferência simples (diária) pelo passo temporal\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Híbrido: Decomposição + KNN time-indexed + Hankel + SVD (SoftImpute) + reconstrução.\n",
    "\n",
    "#     Suposições:\n",
    "#       - df_missing tem colunas 'time' (YYYY-MM-DD HH:MM:SS+00:00) e 'throughput_bps'.\n",
    "#       - Série univariada; index regular ou quase regular.\n",
    "\n",
    "#     Passos:\n",
    "#       1) Índice temporal.\n",
    "#       2) KNN time-indexed para um preenchimento provisório (base local, suave).\n",
    "#       3) Normalização robusta (log1p simétrico + mediana/MAD).\n",
    "#       4) Decomposição sazonal (trend + seasonal) sobre a série *preenchida*.\n",
    "#       5) Resíduo observado = resíduo com NaN nos pontos originalmente faltantes.\n",
    "#       6) Embedding Hankel do resíduo observado (NaNs ficam na matriz).\n",
    "#       7) Matrix completion com SoftImpute (ou IterativeSVD como fallback).\n",
    "#       8) Reconstrução por média de anti-diagonais (hankelização inversa).\n",
    "#       9) Combina: trend + seasonal + residual_svd; denormaliza.\n",
    "#      10) Imputa só onde havia NaN originalmente, com mistura SVD/KNN (blend).\n",
    "#     \"\"\"\n",
    "\n",
    "#     # -------- utilidades internas --------\n",
    "#     def _ensure_dt_index(dfx):\n",
    "#         out = dfx.copy()\n",
    "#         out[\"time\"] = pd.to_datetime(out[\"time\"])\n",
    "#         out = out.set_index(\"time\").sort_index()\n",
    "#         return out\n",
    "\n",
    "#     def _robust_norm(y):\n",
    "#         log_vals = np.sign(y) * np.log1p(np.abs(y))\n",
    "#         med = np.nanmedian(log_vals)\n",
    "#         mad = np.nanmedian(np.abs(log_vals - med))\n",
    "#         z = (log_vals - med) / (mad + 1e-8)\n",
    "#         return pd.Series(z, index=y.index), med, mad\n",
    "\n",
    "#     def _robust_denorm(z, med, mad):\n",
    "#         log_vals = z * (mad + 1e-8) + med\n",
    "#         return np.sign(log_vals) * np.expm1(np.abs(log_vals))\n",
    "\n",
    "#     def _infer_daily_period(idx):\n",
    "#         if not isinstance(idx, pd.DatetimeIndex) or len(idx) < 3:\n",
    "#             return 24\n",
    "#         step_sec = np.median(np.diff(idx.view(\"int64\"))) / 1e9\n",
    "#         step_min = max(1.0, step_sec / 60.0)\n",
    "#         p = int(round(24 * 60 / step_min))\n",
    "#         p = max(2, min(p, max(2, len(idx)//3)))\n",
    "#         return p\n",
    "\n",
    "#     def _decompose(series_z, period):\n",
    "#         filled = series_z.interpolate(method=\"time\", limit_direction=\"both\")\n",
    "#         if period is None:\n",
    "#             period = _infer_daily_period(series_z.index)\n",
    "#         if len(filled) < 3 * max(2, period):\n",
    "#             # fallback simples\n",
    "#             trend = filled.rolling(period, min_periods=1).mean()\n",
    "#             seasonal = (\n",
    "#                 filled.groupby(filled.index.time).transform(\"median\")\n",
    "#                 if isinstance(series_z.index, pd.DatetimeIndex) else\n",
    "#                 pd.Series(0.0, index=filled.index)\n",
    "#             )\n",
    "#             seasonal = seasonal - seasonal.mean()\n",
    "#             resid = filled - trend - seasonal\n",
    "#             return trend, seasonal, resid\n",
    "#         dec = seasonal_decompose(filled, model=\"additive\", period=period, extrapolate_trend=\"freq\")\n",
    "#         return dec.trend, dec.seasonal, dec.resid\n",
    "\n",
    "#     def _knn_timeindexed(series, k):\n",
    "#         # KNN 1D no eixo do tempo (proximidade temporal)\n",
    "#         s = series.copy()\n",
    "#         idx = s.index\n",
    "#         x = (idx.view(\"int64\") // 10**9).astype(np.int64)  # seg desde epoch\n",
    "#         y = s.to_numpy(dtype=float)\n",
    "#         obs = ~np.isnan(y)\n",
    "#         miss = ~obs\n",
    "#         x_obs, y_obs = x[obs], y[obs]\n",
    "#         x_miss = x[miss]\n",
    "#         y_imp = y.copy()\n",
    "#         for i, xm in enumerate(x_miss):\n",
    "#             d = np.abs(x_obs - xm)\n",
    "#             if len(d) <= k: nn = np.argsort(d)\n",
    "#             else: nn = np.argpartition(d, k-1)[:k]\n",
    "#             y_imp[np.where(miss)[0][i]] = np.mean(y_obs[nn])\n",
    "#         return pd.Series(y_imp, index=idx)\n",
    "\n",
    "#     def _build_hankel(x, L):\n",
    "#         # Trajectory/Hankel: shape (L, K) com K = N-L+1, H[i,j] = x[i+j]\n",
    "#         x = np.asarray(x, float)\n",
    "#         N = len(x); K = N - L + 1\n",
    "#         if K <= 0:  # série muito curta p/ L\n",
    "#             L = max(2, min(N, L))\n",
    "#             K = N - L + 1\n",
    "#         H = np.empty((L, K))\n",
    "#         for j in range(K):\n",
    "#             H[:, j] = x[j:j+L]\n",
    "#         return H\n",
    "\n",
    "#     def _diagonal_averaging(H):\n",
    "#         # Reconstrução da série pela média das anti-diagonais\n",
    "#         L, K = H.shape\n",
    "#         N = L + K - 1\n",
    "#         y = np.zeros(N, dtype=float)\n",
    "#         w = np.zeros(N, dtype=float)\n",
    "#         for i in range(L):\n",
    "#             for j in range(K):\n",
    "#                 n = i + j\n",
    "#                 v = H[i, j]\n",
    "#                 if not np.isnan(v):\n",
    "#                     y[n] += v\n",
    "#                     w[n] += 1.0\n",
    "#         y = np.where(w > 0, y / w, np.nan)\n",
    "#         return y\n",
    "\n",
    "#     def _choose_imputer(H):\n",
    "#         frac_nan = np.isnan(H).sum() / H.size\n",
    "#         if frac_nan > 0:\n",
    "#             shrink = max(0.1, min(1.0, frac_nan))\n",
    "#             return SoftImpute(shrinkage_value=shrink)\n",
    "#         # caso raro (sem NaNs): baixa-rank por IterativeSVD\n",
    "#         rank = max(5, int(min(H.shape) * 0.3))\n",
    "#         rank = min(rank, 15, H.shape[0]//2 if H.shape[0]>=2 else 1, H.shape[1]//2 if H.shape[1]>=2 else 1)\n",
    "#         return IterativeSVD(rank=rank)\n",
    "\n",
    "#     # -------- pipeline --------\n",
    "#     df_idx = _ensure_dt_index(df_missing)\n",
    "#     y = df_idx[\"throughput_bps\"].astype(float)\n",
    "#     miss_mask = y.isna()\n",
    "\n",
    "#     # (2) preenchimento provisório por KNN (base local)\n",
    "#     y_knn = _knn_timeindexed(y, k=k)\n",
    "\n",
    "#     # (3) normalização robusta e (4) decomposição sobre a série preenchida\n",
    "#     y_fill = y.copy(); y_fill[miss_mask] = y_knn[miss_mask]\n",
    "#     z_fill, med, mad = _robust_norm(y_fill)\n",
    "#     trend, seasonal, resid = _decompose(z_fill, seasonal_period)\n",
    "\n",
    "#     # (5) resíduo observado: NaN onde originalmente faltava\n",
    "#     resid_obs = resid.copy()\n",
    "#     resid_obs[miss_mask] = np.nan\n",
    "\n",
    "#     # (6) Hankel do resíduo observado (NaNs preservados)\n",
    "#     L = int(max(2, min(window, len(resid_obs) - 1)))\n",
    "#     H = _build_hankel(resid_obs.values, L)\n",
    "\n",
    "#     # (7) Imputação na Hankel via SoftImpute/IterativeSVD\n",
    "#     imputer = _choose_imputer(H)\n",
    "#     H_hat = imputer.fit_transform(H)\n",
    "\n",
    "#     # (8) Reconstrução por média das anti-diagonais -> resíduo refinado\n",
    "#     r_hat = _diagonal_averaging(H_hat)\n",
    "#     r_hat = pd.Series(r_hat, index=resid_obs.index)  # mesmo tamanho da série\n",
    "\n",
    "#     # (9) Reconstrução total em z e denormalização\n",
    "#     z_hat = trend.fillna(0.0) + seasonal.fillna(0.0) + r_hat.fillna(0.0)\n",
    "#     y_svd = _robust_denorm(z_hat, med, mad)\n",
    "\n",
    "#     # (10) Imputar somente onde faltava (mistura SVD + KNN)\n",
    "#     y_final = y.copy()\n",
    "#     y_final[miss_mask] = (1 - blend) * y_svd[miss_mask] + blend * y_knn[miss_mask]\n",
    "\n",
    "    \n",
    "\n",
    "#     out = df_idx.copy()\n",
    "#     out[\"throughput_bps\"] = y_final.values\n",
    "#     return out.reset_index().rename(columns={\"index\": \"time\"})\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fancyimpute import SoftImpute, IterativeSVD\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "def impute_hankel_knn_svd(  # nome mantido p/ compatibilidade\n",
    "    df_missing,\n",
    "    window=168,          # tamanho da janela L da Hankel\n",
    "    k=5,                 # ignorado (mantido por compatibilidade)\n",
    "    blend=0.2,           # mistura final entre SVD e interpolação\n",
    "    seasonal_period=None # None => inferência simples (diária) pelo passo temporal\n",
    "):\n",
    "    \"\"\"\n",
    "    Híbrido: Decomposição + Interpolação temporal linear + Hankel + SVD (SoftImpute) + reconstrução.\n",
    "\n",
    "    Suposições:\n",
    "      - df_missing tem colunas 'time' (YYYY-MM-DD HH:MM:SS+00:00) e 'throughput_bps'.\n",
    "      - Série univariada; index regular ou quase regular.\n",
    "\n",
    "    Passos:\n",
    "      1) Índice temporal.\n",
    "      2) Interpolação temporal linear (both) para preenchimento provisório.\n",
    "      3) Normalização robusta (log1p simétrico + mediana/MAD).\n",
    "      4) Decomposição sazonal (trend + seasonal) sobre a série preenchida.\n",
    "      5) Resíduo observado = resíduo com NaN nos pontos originalmente faltantes.\n",
    "      6) Embedding Hankel do resíduo observado (NaNs ficam na matriz).\n",
    "      7) Matrix completion com SoftImpute (ou IterativeSVD como fallback).\n",
    "      8) Reconstrução por média de anti-diagonais (hankelização inversa).\n",
    "      9) Combina: trend + seasonal + residual_svd; denormaliza.\n",
    "     10) Imputa só onde havia NaN originalmente, com mistura SVD/interpolação (blend).\n",
    "    \"\"\"\n",
    "\n",
    "    # -------- utilidades internas --------\n",
    "    def _ensure_dt_index(dfx):\n",
    "        out = dfx.copy()\n",
    "        out[\"time\"] = pd.to_datetime(out[\"time\"])\n",
    "        out = out.set_index(\"time\").sort_index()\n",
    "        return out\n",
    "\n",
    "    def _robust_norm(y):\n",
    "        log_vals = np.sign(y) * np.log1p(np.abs(y))\n",
    "        med = np.nanmedian(log_vals)\n",
    "        mad = np.nanmedian(np.abs(log_vals - med))\n",
    "        z = (log_vals - med) / (mad + 1e-8)\n",
    "        return pd.Series(z, index=y.index), med, mad\n",
    "\n",
    "    def _robust_denorm(z, med, mad):\n",
    "        log_vals = z * (mad + 1e-8) + med\n",
    "        return np.sign(log_vals) * np.expm1(np.abs(log_vals))\n",
    "\n",
    "    def _infer_daily_period(idx):\n",
    "        if not isinstance(idx, pd.DatetimeIndex) or len(idx) < 3:\n",
    "            return 24\n",
    "        step_sec = np.median(np.diff(idx.view(\"int64\"))) / 1e9\n",
    "        step_min = max(1.0, step_sec / 60.0)\n",
    "        p = int(round(24 * 60 / step_min))\n",
    "        p = max(2, min(p, max(2, len(idx)//3)))\n",
    "        return p\n",
    "\n",
    "    def _decompose(series_z, period):\n",
    "        filled = series_z.interpolate(method=\"time\", limit_direction=\"both\")\n",
    "        if period is None:\n",
    "            period = _infer_daily_period(series_z.index)\n",
    "        if len(filled) < 3 * max(2, period):\n",
    "            trend = filled.rolling(period, min_periods=1).mean()\n",
    "            seasonal = (\n",
    "                filled.groupby(filled.index.time).transform(\"median\")\n",
    "                if isinstance(series_z.index, pd.DatetimeIndex) else\n",
    "                pd.Series(0.0, index=filled.index)\n",
    "            )\n",
    "            seasonal = seasonal - seasonal.mean()\n",
    "            resid = filled - trend - seasonal\n",
    "            return trend, seasonal, resid\n",
    "        dec = seasonal_decompose(filled, model=\"additive\", period=period, extrapolate_trend=\"freq\")\n",
    "        return dec.trend, dec.seasonal, dec.resid\n",
    "\n",
    "    def _build_hankel(x, L):\n",
    "        x = np.asarray(x, float)\n",
    "        N = len(x); K = N - L + 1\n",
    "        if K <= 0:\n",
    "            L = max(2, min(N, L))\n",
    "            K = N - L + 1\n",
    "        H = np.empty((L, K))\n",
    "        for j in range(K):\n",
    "            H[:, j] = x[j:j+L]\n",
    "        return H\n",
    "\n",
    "    def _diagonal_averaging(H):\n",
    "        L, K = H.shape\n",
    "        N = L + K - 1\n",
    "        y = np.zeros(N, dtype=float)\n",
    "        w = np.zeros(N, dtype=float)\n",
    "        for i in range(L):\n",
    "            for j in range(K):\n",
    "                n = i + j\n",
    "                v = H[i, j]\n",
    "                if not np.isnan(v):\n",
    "                    y[n] += v\n",
    "                    w[n] += 1.0\n",
    "        y = np.where(w > 0, y / w, np.nan)\n",
    "        return y\n",
    "\n",
    "    def _choose_imputer(H):\n",
    "        frac_nan = np.isnan(H).sum() / H.size\n",
    "        if frac_nan > 0:\n",
    "            shrink = max(0.1, min(1.0, frac_nan))\n",
    "            return SoftImpute(shrinkage_value=shrink)\n",
    "        rank = max(5, int(min(H.shape) * 0.3))\n",
    "        rank = min(rank, 15, H.shape[0]//2 if H.shape[0]>=2 else 1, H.shape[1]//2 if H.shape[1]>=2 else 1)\n",
    "        return IterativeSVD(rank=rank)\n",
    "\n",
    "    # -------- pipeline --------\n",
    "    df_idx = _ensure_dt_index(df_missing)\n",
    "    y = df_idx[\"throughput_bps\"].astype(float)\n",
    "    miss_mask = y.isna()\n",
    "\n",
    "    # (2) preenchimento provisório por interpolação temporal linear (both)\n",
    "    y_lin = y.interpolate(method=\"time\", limit_direction=\"both\")\n",
    "\n",
    "    # (3) normalização robusta e (4) decomposição sobre a série preenchida\n",
    "    y_fill = y.copy(); y_fill[miss_mask] = y_lin[miss_mask]\n",
    "    z_fill, med, mad = _robust_norm(y_fill)\n",
    "    trend, seasonal, resid = _decompose(z_fill, seasonal_period)\n",
    "\n",
    "    # (5) resíduo observado: NaN onde originalmente faltava\n",
    "    resid_obs = resid.copy()\n",
    "    resid_obs[miss_mask] = np.nan\n",
    "\n",
    "    # (6) Hankel do resíduo observado (NaNs preservados)\n",
    "    L = int(max(2, min(window, len(resid_obs) - 1)))\n",
    "    H = _build_hankel(resid_obs.values, L)\n",
    "\n",
    "    # (7) Imputação na Hankel via SoftImpute/IterativeSVD\n",
    "    imputer = _choose_imputer(H)\n",
    "    H_hat = imputer.fit_transform(H)\n",
    "\n",
    "    # (8) Reconstrução por média das anti-diagonais -> resíduo refinado\n",
    "    r_hat = _diagonal_averaging(H_hat)\n",
    "    r_hat = pd.Series(r_hat, index=resid_obs.index)\n",
    "\n",
    "    # (9) Reconstrução total em z e denormalização\n",
    "    z_hat = trend.fillna(0.0) + seasonal.fillna(0.0) + r_hat.fillna(0.0)\n",
    "    y_svd = _robust_denorm(z_hat, med, mad)\n",
    "\n",
    "    # (10) Imputar somente onde faltava (mistura SVD + interpolação)\n",
    "    y_final = y.copy()\n",
    "    y_final[miss_mask] = (1 - blend) * y_svd[miss_mask] + blend * y_lin[miss_mask]\n",
    "\n",
    "    out = df_idx.copy()\n",
    "    out[\"throughput_bps\"] = y_final.values\n",
    "    return out.reset_index().rename(columns={\"index\": \"time\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f91bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "def impute_linear_interpolation(df_missing):\n",
    "    df_imputed = df_missing.copy()\n",
    "    df_imputed[\"throughput_bps\"] = df_imputed[\"throughput_bps\"].interpolate(\n",
    "        method=\"linear\", limit_direction=\"both\"\n",
    "    )\n",
    "    return df_imputed\n",
    "\n",
    "# def evaluate_imputation(mask_missing, df, df_imputed, method):\n",
    "#     # real and imputed values where theres missing\n",
    "#     y_true = df.loc[mask_missing, \"throughput_bps\"].values\n",
    "#     y_pred = df_imputed.loc[mask_missing, \"throughput_bps\"].values\n",
    "    \n",
    "#     if len(y_true) > 0: \n",
    "#         rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "#         nrmse = rmse / (y_true.max() - y_true.min()) # range\n",
    "#         nrmse_mean = rmse / y_true.mean() # mean\n",
    "#         mae = mean_absolute_error(y_true, y_pred)\n",
    "#         mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "#         r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "#         results.append({\n",
    "#             \"file\": file,\n",
    "#             \"rate\": rate,\n",
    "#             \"method\": method,\n",
    "#             \"rmse\": rmse,\n",
    "#             \"nrmse\": nrmse, # range\n",
    "#             \"nrmse_mean\": nrmse_mean, # mean\n",
    "#             \"mae\": mae,\n",
    "#             \"mape\": mape,\n",
    "#             \"r2\": r2,\n",
    "#         })\n",
    "\n",
    "#         return results\n",
    "\n",
    "# for file, rates_dict in datasets_missing.items():\n",
    "#     for rate, df_missing in rates_dict.items():\n",
    "#         mask_missing = df_missing[\"throughput_bps\"].isna()\n",
    "        \n",
    "#         # linear interpolation\n",
    "#         df_imputed = impute_linear_interpolation(df_missing)\n",
    "#         results = evaluate_imputation(mask_missing, df, df_imputed, \"linear interpolation\")\n",
    "\n",
    "#         df_imputed_knn = impute_knn_imputer(df_missing, k=5)\n",
    "#         results = evaluate_imputation(mask_missing, df, df_imputed_knn, \"knn imputer (k=5)\")\n",
    "\n",
    "#         # 4) Kalman - ARIMA (ex.: ARIMA(1,1,1); sem sazonalidade)\n",
    "#         df_kalman_arima = impute_kalman(\n",
    "#             df_missing,\n",
    "#             model=\"arima\",\n",
    "#             arima_order=(1, 1, 1),\n",
    "#             seasonal_order=(0, 0, 0, 0)\n",
    "#         )\n",
    "#         results = evaluate_imputation(mask_missing, df, df_kalman_arima, \"kalman arima (1,1,1)\")\n",
    "\n",
    "#         # 5) Kalman - Modelo Estrutural (nível local + sazonal diária de 24, se fizer sentido)\n",
    "#         df_kalman_struct = impute_kalman(\n",
    "#             df_missing,\n",
    "#             model=\"structural\",\n",
    "#             level=\"local level\",\n",
    "#             seasonal_period=None  # ou 24, 7*24 etc., conforme seu dado\n",
    "#         )\n",
    "\n",
    "#         # df_sma = impute_moving_average(df_missing, window=5, center=True)\n",
    "#         # results = evaluate_imputation(mask_missing, df, df_sma, \"moving average (win=5, centered)\")\n",
    "\n",
    "#         # EWMA (alpha 0.2)\n",
    "#         df_ewma = impute_ewma(df_missing, alpha=0.2)\n",
    "#         results = evaluate_imputation(mask_missing, df, df_ewma, \"ewma (alpha=0.2)\")\n",
    "\n",
    "#         results = evaluate_imputation(mask_missing, df, df_kalman_struct, \"kalman structural (level)\")\n",
    "        \n",
    "#         df_hankel = impute_hankel_knn_svd(df_missing, window=72, k=5, blend=0.2, seasonal_period=None)\n",
    "#         results = evaluate_imputation(mask_missing, df, df_hankel, \"hankel+knn+svd (L=72, k=5, λ=0.2)\")\n",
    "\n",
    "#         df_imputed, diag = impute_throughput_svd_knn(\n",
    "#                 df_missing,\n",
    "#                 col=\"throughput_bps\",\n",
    "#                 min_period=24,   # adjust for your sampling rate\n",
    "#                 max_period=1000, # search window for period detection\n",
    "#                 energy=0.9,\n",
    "#                 k=5,\n",
    "#                 allow_future=True\n",
    "#             )\n",
    "\n",
    "#         # Merge back into DataFrame with same structure\n",
    "#         df_result = df_missing.copy()\n",
    "#         df_result[\"throughput_bps\"] = df_imputed.values\n",
    "\n",
    "#         # --------------------------\n",
    "#         # Evaluate\n",
    "#         # --------------------------\n",
    "#         results = evaluate_imputation(\n",
    "#             mask_missing, \n",
    "#             df, df_result, \n",
    "#             method=\"Hankel+SVD+KNN\"\n",
    "#         )\n",
    "        \n",
    "# df_results = pd.DataFrame(results)\n",
    "# df_results.head()\n",
    "# df_results.to_csv(\"results.csv\", index=False)\n",
    "\n",
    "# print(\"Resultados salvos em results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9f18a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_temporal_matrix(df, n_lines=100):\n",
    "#     throughput_vector = df[\"throughput_bps\"].values\n",
    "#     columns_quantity = df.shape[1]\n",
    "#     temporal_matrix = (\n",
    "#         throughput_vector[: columns_quantity * n_lines]\n",
    "#         .reshape(columns_quantity, n_lines)\n",
    "#         .T\n",
    "#     )\n",
    "#     return temporal_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5ff34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "results = []\n",
    "\n",
    "def evaluate_imputation(mask_missing, df_true, df_imp, method, file, rate):\n",
    "    y_true = df_true.loc[mask_missing, \"throughput_bps\"].values\n",
    "    y_pred = df_imp.loc[mask_missing, \"throughput_bps\"].values\n",
    "\n",
    "    if y_true.size == 0:\n",
    "        return results\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    nrmse = rmse / (y_true.max() - y_true.min())\n",
    "    nrmse_mean = rmse / y_true.mean()\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    results.append({\n",
    "        \"file\": file,\n",
    "        \"rate\": int(rate),\n",
    "        \"method\": method,\n",
    "        \"rmse\": rmse,\n",
    "        \"nrmse\": nrmse,\n",
    "        \"nrmse_mean\": nrmse_mean,\n",
    "        \"mae\": mae,\n",
    "        \"mape\": mape,\n",
    "        \"r2\": r2,\n",
    "    })\n",
    "    return results\n",
    "\n",
    "datasets_original = {}\n",
    "datasets_missing = {}\n",
    "\n",
    "for file in os.listdir(BASE_DIR):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(BASE_DIR, file))\n",
    "        \n",
    "        # salva o dataset original\n",
    "        base_key = file.removesuffix(\".throughput.csv\")  \n",
    "        datasets_original[base_key] = df.copy()  \n",
    "\n",
    "        # gera versões com missing\n",
    "        datasets_missing[base_key] = {}\n",
    "        for rate in [0.1, 0.2, 0.3, 0.4]:\n",
    "            df_missing = introduce_missing_data(df, missing_rate=rate, seed=42)\n",
    "            rate_key = str(int(rate * 100))\n",
    "            datasets_missing[base_key][rate_key] = df_missing\n",
    "\n",
    "for file, rates_dict in datasets_missing.items():\n",
    "    df = datasets_original[file].copy()\n",
    "\n",
    "    for rate_key, df_missing in rates_dict.items():\n",
    "        rate = int(rate_key)\n",
    "        mask_missing = df_missing[\"throughput_bps\"].isna()\n",
    "\n",
    "        df_lin = impute_linear_interpolation(df_missing)\n",
    "        evaluate_imputation(mask_missing, df, df_lin, \"linear interpolation\", file, rate)\n",
    "\n",
    "        df_knn = impute_knn_imputer(df_missing, k=5)\n",
    "        evaluate_imputation(mask_missing, df, df_knn, \"knn imputer (k=5)\", file, rate)\n",
    "\n",
    "        df_kalman_arima = impute_kalman(\n",
    "            df_missing,\n",
    "            model=\"arima\",\n",
    "            arima_order=(1, 1, 1),\n",
    "            seasonal_order=(0, 0, 0, 0)\n",
    "        )\n",
    "        evaluate_imputation(mask_missing, df, df_kalman_arima, \"kalman arima (1,1,1)\", file, rate)\n",
    "\n",
    "        df_kalman_struct = impute_kalman(\n",
    "            df_missing,\n",
    "            model=\"structural\",\n",
    "            level=\"local level\",\n",
    "            seasonal_period=None\n",
    "        )\n",
    "        evaluate_imputation(mask_missing, df, df_kalman_struct, \"kalman structural (level)\", file, rate)\n",
    "\n",
    "        df_ewma = impute_ewma(df_missing, alpha=0.2)\n",
    "        evaluate_imputation(mask_missing, df, df_ewma, \"ewma (alpha=0.2)\", file, rate)\n",
    "\n",
    "        df_hankel = impute_hankel_knn_svd(df_missing, window=72, k=5, blend=0.2, seasonal_period=None)\n",
    "        evaluate_imputation(mask_missing, df, df_hankel, \"hankel+knn+svd (L=72, k=5, λ=0.2)\", file, rate)\n",
    "\n",
    "        df_imp_series, diag = impute_throughput_svd_knn(\n",
    "            df_missing,\n",
    "            col=\"throughput_bps\",\n",
    "            min_period=24,\n",
    "            max_period=1000,\n",
    "            energy=0.9,\n",
    "            k=5,\n",
    "            allow_future=True\n",
    "        )\n",
    "        df_result = df_missing.copy()\n",
    "        df_result[\"throughput_bps\"] = df_imp_series.values\n",
    "        evaluate_imputation(mask_missing, df, df_result, \"Hankel+SVD+KNN\", file, rate)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(\"results.csv\", index=False)\n",
    "print(\"Resultados salvos em results.csv\")\n",
    "df_results.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
