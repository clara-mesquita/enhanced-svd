{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c12f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# ----------------------------\n",
    "# 1) PERIOD (n_lines) ESTIMATION\n",
    "# ----------------------------\n",
    "\n",
    "def _acf(y: np.ndarray, max_lag: int) -> np.ndarray: # ->basicamente isso aqui calcula a galera com maior correlação e deixa em uma só linha\n",
    "    \"\"\"Biased ACF up to max_lag (lag 0..max_lag). NaNs are linearly interpolated first.\"\"\"\n",
    "    y = pd.Series(y).interpolate(limit_direction=\"both\").to_numpy()\n",
    "    y = y - np.nanmean(y)\n",
    "    n = len(y)\n",
    "    acf_vals = np.empty(max_lag + 1)\n",
    "    denom = np.dot(y, y) + 1e-12\n",
    "    for lag in range(max_lag + 1):\n",
    "        acf_vals[lag] = np.dot(y[: n - lag], y[lag:]) / denom\n",
    "    return acf_vals\n",
    "\n",
    "def _fft_period(y: np.ndarray, min_period: int, max_period: int) -> Optional[int]: # isso aqui faz a mesma coisa só que com fourrier \n",
    "    \"\"\"FFT-based dominant period in [min_period, max_period], None if not found.\"\"\"\n",
    "    y = pd.Series(y).interpolate(limit_direction=\"both\").to_numpy()\n",
    "    y = y - np.mean(y)\n",
    "    n = len(y)\n",
    "    if n < 4:\n",
    "        return None\n",
    "    # Real FFT spectrum\n",
    "    spec = np.fft.rfft(y)\n",
    "    freqs = np.fft.rfftfreq(n, d=1.0)  # assume unit sampling\n",
    "    # Exclude DC\n",
    "    mask = freqs > 0\n",
    "    freqs = freqs[mask]\n",
    "    power = (spec[mask].real**2 + spec[mask].imag**2)\n",
    "    # Convert freq -> period\n",
    "    periods = np.round(1.0 / freqs).astype(int)\n",
    "    # Keep only within bounds\n",
    "    sel = (periods >= min_period) & (periods <= max_period)\n",
    "    if not np.any(sel):\n",
    "        return None\n",
    "    # Aggregate power by period (many freqs can map to same rounded period)\n",
    "    dfp = pd.DataFrame({\"period\": periods[sel], \"power\": power[sel]})\n",
    "    top = dfp.groupby(\"period\", as_index=False)[\"power\"].sum().sort_values(\"power\", ascending=False)\n",
    "    return int(top[\"period\"].iloc[0]) if len(top) else None\n",
    "\n",
    "def estimate_period(\n",
    "    y: np.ndarray,\n",
    "    min_period: int = 4,\n",
    "    max_period: Optional[int] = None\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Pick period (n_lines) automatically using ACF peak with FFT fallback.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    n = len(y)\n",
    "    if max_period is None:\n",
    "        max_period = max(7, min(n // 4, 1000))  # sensible cap\n",
    "\n",
    "    if n < min_period * 2:\n",
    "        # too short — just return something small\n",
    "        return max(min_period, min(n, 8))\n",
    "\n",
    "    acf_vals = _acf(y, max_period)\n",
    "    # Ignore lag 0; pick the best lag in [min_period, max_period]\n",
    "    candidate_lags = np.arange(min_period, max_period + 1)\n",
    "    best_lag = candidate_lags[np.argmax(acf_vals[min_period: max_period + 1])]\n",
    "\n",
    "    # FFT fallback check: if ACF peak is weak, try FFT suggestion\n",
    "    acf_strength = acf_vals[best_lag]\n",
    "    fft_suggestion = _fft_period(y, min_period, max_period)\n",
    "    if fft_suggestion is not None:\n",
    "        if acf_strength < 0.15:  # weak ACF; trust FFT\n",
    "            return int(fft_suggestion)\n",
    "        # If both agree closely, prefer the smaller (more stable) period\n",
    "        if abs(fft_suggestion - best_lag) <= 2:\n",
    "            return int(min(fft_suggestion, best_lag))\n",
    "    return int(best_lag)\n",
    "\n",
    "# ---------------------------------\n",
    "# 2) FOLDING (PERIODIC TEMPORAL MATRIX)\n",
    "# ---------------------------------\n",
    "\n",
    "def fold_series_to_matrix(y: np.ndarray, period: int) -> Tuple[np.ndarray, int]: # basicamente isso cria a matriz temporal \n",
    "    \"\"\"\n",
    "    Fold 1D series into a (n_blocks, period) matrix. \n",
    "    Pads the last block with NaN if needed.\n",
    "    Returns (M, original_len).\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    n = len(y)\n",
    "    n_blocks = int(np.ceil(n / period)) #n de coluans\n",
    "    pad_len = n_blocks * period - n\n",
    "    if pad_len > 0:\n",
    "        y = np.concatenate([y, np.full(pad_len, np.nan)])\n",
    "    M = y.reshape(n_blocks, period)\n",
    "    return M, n\n",
    "\n",
    "def unfold_matrix_to_series(M: np.ndarray, original_len: int) -> np.ndarray: # volta pro formato original\n",
    "    \"\"\"Inverse of fold: row-wise flatten and trim to original length.\"\"\"\n",
    "    y = M.reshape(-1)\n",
    "    return y[:original_len]\n",
    "\n",
    "# ----------------------------\n",
    "# 3) SVD + RANK SELECTION\n",
    "# ----------------------------\n",
    "\n",
    "def svd_rank(M_filled: np.ndarray, energy: float = 0.9) -> Tuple[np.ndarray, np.ndarray, np.ndarray, int]: # faz o svd na tora e escolhe o r com a soma dos valore sisnuglares so deus sabe pq\n",
    "    \"\"\"\n",
    "    Compute SVD and choose rank r by cumulative explained 'energy' (sum of singular values).\n",
    "    \"\"\"\n",
    "    U, s, Vt = np.linalg.svd(M_filled, full_matrices=False)\n",
    "    cum = np.cumsum(s) / (np.sum(s) + 1e-12)\n",
    "    r = int(np.searchsorted(cum, energy) + 1)\n",
    "    r = max(1, min(r, min(M_filled.shape)))\n",
    "    return U, s, Vt, r\n",
    "\n",
    "# -------------------------------------------\n",
    "# 4) KNN IMPUTE USING ROW EMBEDDINGS FROM SVD\n",
    "# -------------------------------------------\n",
    "\n",
    "def _warm_start_fill(M: np.ndarray) -> np.ndarray: # coloca mediana em tudo e vapo só pra começar\n",
    "    \"\"\"Column-wise median fill as a stable warm start.\"\"\"\n",
    "    M_filled = M.copy()\n",
    "    col_medians = np.nanmedian(M_filled, axis=0)\n",
    "    # If an entire column is NaN, fallback to global median\n",
    "    if np.any(np.isnan(col_medians)):\n",
    "        global_med = np.nanmedian(M_filled)\n",
    "        col_medians = np.where(np.isnan(col_medians), global_med, col_medians)\n",
    "    inds = np.where(np.isnan(M_filled))\n",
    "    M_filled[inds] = np.take(col_medians, inds[1])\n",
    "    return M_filled\n",
    "\n",
    "def impute_with_knn_in_latent( \n",
    "    M: np.ndarray,\n",
    "    k: int = 5,\n",
    "    energy: float = 0.9,\n",
    "    allow_future: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Impute NaNs in M by KNN in SVD latent space (rows ≈ cycles/weeks). que porra eh latent space\n",
    "    For each missing cell (i,j), find k nearest rows to row i in latent space\n",
    "    among those with M[row, j] observed (and optionally row < i).\n",
    "    \"\"\"\n",
    "    M_filled0 = _warm_start_fill(M)\n",
    "    U, s, Vt, r = svd_rank(M_filled0, energy=energy)\n",
    "    # Row embeddings (T x r): U_r * S_r\n",
    "    Z = U[:, :r] * s[:r]  # broadcasting: each column of U scaled by s\n",
    "\n",
    "    M_imp = M.copy()\n",
    "    T, P = M.shape\n",
    "    eps = 1e-8\n",
    "\n",
    "    # Precompute which rows have each column observed\n",
    "    observed_mask = ~np.isnan(M)\n",
    "    for i in range(T):\n",
    "        # indices (columns) that are missing in row i\n",
    "        miss_cols = np.where(~observed_mask[i])[0]\n",
    "        if len(miss_cols) == 0:\n",
    "            continue\n",
    "        zi = Z[i]\n",
    "\n",
    "        # Candidate rows for neighbors (global, filtered below per col)\n",
    "        if allow_future:\n",
    "            candidate_rows_global = np.arange(T)\n",
    "        else:\n",
    "            candidate_rows_global = np.arange(0, i)  # only past\n",
    "\n",
    "        if len(candidate_rows_global) == 0:\n",
    "            # If we can't use past (i==0), allow future just for this row:\n",
    "            candidate_rows_global = np.arange(T)\n",
    "\n",
    "        # Distances in latent space to all candidates\n",
    "        Zc = Z[candidate_rows_global]\n",
    "        dists = np.linalg.norm(Zc - zi[None, :], axis=1)\n",
    "        dists = dists + eps  # avoid zero\n",
    "\n",
    "        for j in miss_cols:\n",
    "            # keep only candidates that have this column observed\n",
    "            obs_rows = candidate_rows_global[observed_mask[candidate_rows_global, j]]\n",
    "            if len(obs_rows) == 0:\n",
    "                # fall back to warm-start value if nothing observed\n",
    "                M_imp[i, j] = M_filled0[i, j]\n",
    "                continue\n",
    "\n",
    "            # distances for those rows\n",
    "            d = np.linalg.norm(Z[obs_rows] - zi[None, :], axis=1) + eps\n",
    "            # k nearest\n",
    "            if len(d) > k:\n",
    "                idx = np.argpartition(d, k)[:k]\n",
    "                nn_rows = obs_rows[idx]\n",
    "                d = d[idx]\n",
    "            else:\n",
    "                nn_rows = obs_rows\n",
    "\n",
    "            w = 1.0 / d  # inverse-distance weights\n",
    "            vals = M[nn_rows, j]\n",
    "            # safety: if still NaN (shouldn't happen), drop them\n",
    "            ok = ~np.isnan(vals)\n",
    "            if not np.any(ok):\n",
    "                M_imp[i, j] = M_filled0[i, j]\n",
    "            else:\n",
    "                vals = vals[ok]\n",
    "                w = w[ok]\n",
    "                M_imp[i, j] = np.sum(w * vals) / np.sum(w)\n",
    "\n",
    "    return M_imp\n",
    "\n",
    "# ----------------------------\n",
    "# 5) MAIN PIPELINE\n",
    "# ----------------------------\n",
    "\n",
    "def impute_throughput_svd_knn(\n",
    "    df: pd.DataFrame,\n",
    "    col: str = \"throughput_bps\",\n",
    "    min_period: int = 4,\n",
    "    max_period: Optional[int] = None,\n",
    "    energy: float = 0.9,\n",
    "    k: int = 5,\n",
    "    allow_future: bool = True\n",
    ") -> Tuple[pd.Series, dict]:\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "    1) auto period detection -> n_lines (period)\n",
    "    2) fold into (n_blocks, period) matrix\n",
    "    3) SVD -> latent row embeddings\n",
    "    4) KNN in latent space to impute NaNs\n",
    "    5) unfold back to 1D series\n",
    "    Returns (imputed_series, diagnostics).\n",
    "    \"\"\"\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column '{col}' not found in df.\")\n",
    "\n",
    "    y = df[col].to_numpy(dtype=float)\n",
    "    original_index = df.index\n",
    "\n",
    "    # Detect period (n_lines)\n",
    "    period = estimate_period(y, min_period=min_period, max_period=max_period)\n",
    "\n",
    "    # Fold\n",
    "    M, orig_len = fold_series_to_matrix(y, period=period)\n",
    "\n",
    "    # Impute in latent KNN space\n",
    "    M_imp = impute_with_knn_in_latent(M, k=k, energy=energy, allow_future=allow_future)\n",
    "\n",
    "    # Unfold\n",
    "    y_imp = unfold_matrix_to_series(M_imp, original_len=orig_len)\n",
    "\n",
    "    print(y_imp)\n",
    "\n",
    "    diagnostics = {\n",
    "        \"period_estimated\": period,\n",
    "        \"matrix_shape\": M.shape,\n",
    "        \"rank_energy_target\": energy,\n",
    "        \"k_neighbors\": k,\n",
    "        \"allow_future\": allow_future,\n",
    "    }\n",
    "    return pd.Series(y_imp, index=original_index, name=f\"{col}_imputed\"), diagnostics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8027dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduced 10.0% missing data in 6717 rows.\n",
      "[ 6291904.17111111  7446471.12444444 10027996.31194444 ...\n",
      " 18689784.75972222 11790824.49611111 13256918.35777778]\n",
      "Introduced 20.0% missing data in 6717 rows.\n",
      "[ 6291904.17111111  7446471.12444444 10027996.31194444 ...\n",
      " 18689784.75972222 11790824.49611111 13256918.35777778]\n",
      "Introduced 30.0% missing data in 6717 rows.\n",
      "[ 6291904.17111111  7446471.12444444 10027996.31194444 ...\n",
      " 18689784.75972222 11790824.49611111 13256918.35777778]\n",
      "Introduced 40.0% missing data in 6717 rows.\n",
      "[ 6291904.17111111  7446471.12444444 10027996.31194444 ...\n",
      " 18689784.75972222 11790824.49611111 13256918.35777778]\n",
      "Introduced 10.0% missing data in 6717 rows.\n",
      "[8276379.73805556 6299608.46222222 7835900.39555556 ... 8620657.81083333\n",
      " 6094621.17694444 6457880.36583333]\n",
      "Introduced 20.0% missing data in 6717 rows.\n",
      "[8276379.73805556 6299608.46222222 7835900.39555556 ... 8620657.81083333\n",
      " 6094621.17694444 6457880.36583333]\n",
      "Introduced 30.0% missing data in 6717 rows.\n",
      "[8276379.73805556 6299608.46222222 7835900.39555556 ... 8620657.81083333\n",
      " 6094621.17694444 6457880.36583333]\n",
      "Introduced 40.0% missing data in 6717 rows.\n",
      "[8276379.73805556 6299608.46222222 7835900.39555556 ... 8620657.81083333\n",
      " 6094621.17694444 6457880.36583333]\n",
      "Introduced 10.0% missing data in 6717 rows.\n",
      "[34754730.72083333 24625643.12472222 35204588.04166666 ...\n",
      " 18308426.29277778 21497392.24138889 27547991.83055555]\n",
      "Introduced 20.0% missing data in 6717 rows.\n",
      "[34754730.72083333 24625643.12472222 35204588.04166666 ...\n",
      " 18308426.29277778 21497392.24138889 27547991.83055555]\n",
      "Introduced 30.0% missing data in 6717 rows.\n",
      "[34754730.72083333 24625643.12472222 35204588.04166666 ...\n",
      " 18308426.29277778 21497392.24138889 27547991.83055555]\n",
      "Introduced 40.0% missing data in 6717 rows.\n",
      "[34754730.72083333 24625643.12472222 35204588.04166666 ...\n",
      " 18308426.29277778 21497392.24138889 27547991.83055555]\n",
      "Resultados salvos em results.csv\n",
      "               file  rate          method          rmse     nrmse  \\\n",
      "0  0.throughput.csv   0.1  Hankel+SVD+KNN  3.561118e+06  0.072935   \n",
      "1  0.throughput.csv   0.2  Hankel+SVD+KNN  3.853857e+06  0.076111   \n",
      "2  0.throughput.csv   0.3  Hankel+SVD+KNN  4.205007e+06  0.079168   \n",
      "3  0.throughput.csv   0.4  Hankel+SVD+KNN  4.598702e+06  0.085925   \n",
      "4  1.throughput.csv   0.1  Hankel+SVD+KNN  6.481514e+06  0.072368   \n",
      "\n",
      "            mae        r2  \n",
      "0  2.678660e+06  0.878516  \n",
      "1  2.936385e+06  0.855333  \n",
      "2  3.200958e+06  0.825895  \n",
      "3  3.500805e+06  0.796232  \n",
      "4  3.233965e+06  0.498844  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Assume you imported/improved the impute_throughput_svd_knn from the prototype code\n",
    "# from svd_knn_pipeline import impute_throughput_svd_knn   # example import\n",
    "\n",
    "BASE_DIR = \"./test-data\"\n",
    "\n",
    "# --------------------------\n",
    "# Inject Missing Data\n",
    "# --------------------------\n",
    "def introduce_missing_data(df, missing_rate, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df_missing = df.copy()\n",
    "    mask = rng.random(len(df_missing)) < missing_rate\n",
    "    df_missing.loc[mask, \"throughput_bps\"] = np.nan\n",
    "    print(f\"Introduced {missing_rate * 100}% missing data in {len(mask)} rows.\")\n",
    "    return df_missing, mask   # return also the mask to evaluate later\n",
    "\n",
    "# --------------------------\n",
    "# Evaluation Function\n",
    "# --------------------------\n",
    "def evaluate_imputation(mask_missing, df_true, df_imputed, method, file, rate, results):\n",
    "    y_true = df_true.loc[mask_missing, \"throughput_bps\"].values\n",
    "    y_pred = df_imputed.loc[mask_missing, \"throughput_bps_imputed\"].values\n",
    "\n",
    "    if len(y_true) > 0:\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        nrmse = rmse / (y_true.max() - y_true.min())  # normalized by range\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "        results.append({\n",
    "            \"file\": file,\n",
    "            \"rate\": rate,\n",
    "            \"method\": method,\n",
    "            \"rmse\": rmse,\n",
    "            \"nrmse\": nrmse,\n",
    "            \"mae\": mae,\n",
    "            \"r2\": r2,\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# --------------------------\n",
    "# Main Loop\n",
    "# --------------------------\n",
    "datasets_missing = {}\n",
    "results = []\n",
    "\n",
    "for file in os.listdir(BASE_DIR):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(BASE_DIR, file))\n",
    "        base_key = file.removesuffix(\".throughput.csv\")\n",
    "        datasets_missing[base_key] = {}\n",
    "\n",
    "        for rate in [0.1, 0.2, 0.3, 0.4]:\n",
    "            # introduce missing\n",
    "            df_missing, mask_missing = introduce_missing_data(df, missing_rate=rate, seed=42)\n",
    "            rate_key = f\"{int(rate * 100)}\"\n",
    "            datasets_missing[base_key][rate_key] = df_missing\n",
    "\n",
    "            # --------------------------\n",
    "            # Apply your SVD+KNN pipeline\n",
    "            # --------------------------\n",
    "            df_imputed, diag = impute_throughput_svd_knn(\n",
    "                df_missing,\n",
    "                col=\"throughput_bps\",\n",
    "                min_period=24,   # adjust for your sampling rate\n",
    "                max_period=1000, # search window for period detection\n",
    "                energy=0.9,\n",
    "                k=5,\n",
    "                allow_future=True\n",
    "            )\n",
    "\n",
    "            # Merge back into DataFrame with same structure\n",
    "            df_result = df_missing.copy()\n",
    "            df_result[\"throughput_bps_imputed\"] = df_imputed.values\n",
    "\n",
    "            # --------------------------\n",
    "            # Evaluate\n",
    "            # --------------------------\n",
    "            results = evaluate_imputation(\n",
    "                mask_missing, \n",
    "                df, df_result, \n",
    "                method=\"Hankel+SVD+KNN\", \n",
    "                file=file, \n",
    "                rate=rate, \n",
    "                results=results\n",
    "            )\n",
    "\n",
    "# --------------------------\n",
    "# Save Results\n",
    "# --------------------------\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(\"results.csv\", index=False)\n",
    "print(\"Resultados salvos em results.csv\")\n",
    "print(df_results.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
