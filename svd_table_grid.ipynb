{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ca6494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import KNNImputer\n",
    "from fancyimpute import SoftImpute, IterativeSVD\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5458464",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"./test-data\" # dir with datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e1dd50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.throughput.csv': {'10.0%':                            time  throughput_bps\n",
       "  0     2023-10-09 00:00:00+00:00    6.291904e+06\n",
       "  1     2023-10-09 01:00:00+00:00    7.446471e+06\n",
       "  2     2023-10-09 02:00:00+00:00    1.002800e+07\n",
       "  3     2023-10-09 03:00:00+00:00    1.438063e+07\n",
       "  4     2023-10-09 04:00:00+00:00             NaN\n",
       "  ...                         ...             ...\n",
       "  6712  2024-07-14 17:00:00+00:00    2.346846e+07\n",
       "  6713  2024-07-14 18:00:00+00:00    2.124341e+07\n",
       "  6714  2024-07-14 19:00:00+00:00    1.868978e+07\n",
       "  6715  2024-07-14 20:00:00+00:00    1.179082e+07\n",
       "  6716  2024-07-14 21:00:00+00:00    1.325692e+07\n",
       "  \n",
       "  [6717 rows x 2 columns],\n",
       "  '20.0%':                            time  throughput_bps\n",
       "  0     2023-10-09 00:00:00+00:00    6.291904e+06\n",
       "  1     2023-10-09 01:00:00+00:00    7.446471e+06\n",
       "  2     2023-10-09 02:00:00+00:00    1.002800e+07\n",
       "  3     2023-10-09 03:00:00+00:00    1.438063e+07\n",
       "  4     2023-10-09 04:00:00+00:00             NaN\n",
       "  ...                         ...             ...\n",
       "  6712  2024-07-14 17:00:00+00:00    2.346846e+07\n",
       "  6713  2024-07-14 18:00:00+00:00    2.124341e+07\n",
       "  6714  2024-07-14 19:00:00+00:00    1.868978e+07\n",
       "  6715  2024-07-14 20:00:00+00:00    1.179082e+07\n",
       "  6716  2024-07-14 21:00:00+00:00    1.325692e+07\n",
       "  \n",
       "  [6717 rows x 2 columns],\n",
       "  '30.0%':                            time  throughput_bps\n",
       "  0     2023-10-09 00:00:00+00:00    6.291904e+06\n",
       "  1     2023-10-09 01:00:00+00:00    7.446471e+06\n",
       "  2     2023-10-09 02:00:00+00:00    1.002800e+07\n",
       "  3     2023-10-09 03:00:00+00:00    1.438063e+07\n",
       "  4     2023-10-09 04:00:00+00:00             NaN\n",
       "  ...                         ...             ...\n",
       "  6712  2024-07-14 17:00:00+00:00    2.346846e+07\n",
       "  6713  2024-07-14 18:00:00+00:00    2.124341e+07\n",
       "  6714  2024-07-14 19:00:00+00:00    1.868978e+07\n",
       "  6715  2024-07-14 20:00:00+00:00    1.179082e+07\n",
       "  6716  2024-07-14 21:00:00+00:00    1.325692e+07\n",
       "  \n",
       "  [6717 rows x 2 columns],\n",
       "  '40.0%':                            time  throughput_bps\n",
       "  0     2023-10-09 00:00:00+00:00    6.291904e+06\n",
       "  1     2023-10-09 01:00:00+00:00    7.446471e+06\n",
       "  2     2023-10-09 02:00:00+00:00    1.002800e+07\n",
       "  3     2023-10-09 03:00:00+00:00    1.438063e+07\n",
       "  4     2023-10-09 04:00:00+00:00             NaN\n",
       "  ...                         ...             ...\n",
       "  6712  2024-07-14 17:00:00+00:00             NaN\n",
       "  6713  2024-07-14 18:00:00+00:00    2.124341e+07\n",
       "  6714  2024-07-14 19:00:00+00:00    1.868978e+07\n",
       "  6715  2024-07-14 20:00:00+00:00    1.179082e+07\n",
       "  6716  2024-07-14 21:00:00+00:00    1.325692e+07\n",
       "  \n",
       "  [6717 rows x 2 columns]},\n",
       " '1.throughput.csv': {'10.0%':                            time  throughput_bps\n",
       "  0     2023-10-09 00:00:00+00:00    8.276380e+06\n",
       "  1     2023-10-09 01:00:00+00:00    6.299608e+06\n",
       "  2     2023-10-09 02:00:00+00:00    7.835900e+06\n",
       "  3     2023-10-09 03:00:00+00:00    1.360294e+07\n",
       "  4     2023-10-09 04:00:00+00:00             NaN\n",
       "  ...                         ...             ...\n",
       "  6712  2024-07-14 17:00:00+00:00    1.009773e+07\n",
       "  6713  2024-07-14 18:00:00+00:00    9.807295e+06\n",
       "  6714  2024-07-14 19:00:00+00:00    8.620658e+06\n",
       "  6715  2024-07-14 20:00:00+00:00    6.094621e+06\n",
       "  6716  2024-07-14 21:00:00+00:00    6.457880e+06\n",
       "  \n",
       "  [6717 rows x 2 columns],\n",
       "  '20.0%':                            time  throughput_bps\n",
       "  0     2023-10-09 00:00:00+00:00    8.276380e+06\n",
       "  1     2023-10-09 01:00:00+00:00    6.299608e+06\n",
       "  2     2023-10-09 02:00:00+00:00    7.835900e+06\n",
       "  3     2023-10-09 03:00:00+00:00    1.360294e+07\n",
       "  4     2023-10-09 04:00:00+00:00             NaN\n",
       "  ...                         ...             ...\n",
       "  6712  2024-07-14 17:00:00+00:00    1.009773e+07\n",
       "  6713  2024-07-14 18:00:00+00:00    9.807295e+06\n",
       "  6714  2024-07-14 19:00:00+00:00    8.620658e+06\n",
       "  6715  2024-07-14 20:00:00+00:00    6.094621e+06\n",
       "  6716  2024-07-14 21:00:00+00:00    6.457880e+06\n",
       "  \n",
       "  [6717 rows x 2 columns],\n",
       "  '30.0%':                            time  throughput_bps\n",
       "  0     2023-10-09 00:00:00+00:00    8.276380e+06\n",
       "  1     2023-10-09 01:00:00+00:00    6.299608e+06\n",
       "  2     2023-10-09 02:00:00+00:00    7.835900e+06\n",
       "  3     2023-10-09 03:00:00+00:00    1.360294e+07\n",
       "  4     2023-10-09 04:00:00+00:00             NaN\n",
       "  ...                         ...             ...\n",
       "  6712  2024-07-14 17:00:00+00:00    1.009773e+07\n",
       "  6713  2024-07-14 18:00:00+00:00    9.807295e+06\n",
       "  6714  2024-07-14 19:00:00+00:00    8.620658e+06\n",
       "  6715  2024-07-14 20:00:00+00:00    6.094621e+06\n",
       "  6716  2024-07-14 21:00:00+00:00    6.457880e+06\n",
       "  \n",
       "  [6717 rows x 2 columns],\n",
       "  '40.0%':                            time  throughput_bps\n",
       "  0     2023-10-09 00:00:00+00:00    8.276380e+06\n",
       "  1     2023-10-09 01:00:00+00:00    6.299608e+06\n",
       "  2     2023-10-09 02:00:00+00:00    7.835900e+06\n",
       "  3     2023-10-09 03:00:00+00:00    1.360294e+07\n",
       "  4     2023-10-09 04:00:00+00:00             NaN\n",
       "  ...                         ...             ...\n",
       "  6712  2024-07-14 17:00:00+00:00             NaN\n",
       "  6713  2024-07-14 18:00:00+00:00    9.807295e+06\n",
       "  6714  2024-07-14 19:00:00+00:00    8.620658e+06\n",
       "  6715  2024-07-14 20:00:00+00:00    6.094621e+06\n",
       "  6716  2024-07-14 21:00:00+00:00    6.457880e+06\n",
       "  \n",
       "  [6717 rows x 2 columns]},\n",
       " '2.throughput.csv': {'10.0%':                            time  throughput_bps\n",
       "  0     2023-10-09 00:00:00+00:00    3.475473e+07\n",
       "  1     2023-10-09 01:00:00+00:00    2.462564e+07\n",
       "  2     2023-10-09 02:00:00+00:00    3.520459e+07\n",
       "  3     2023-10-09 03:00:00+00:00    1.839942e+07\n",
       "  4     2023-10-09 04:00:00+00:00             NaN\n",
       "  ...                         ...             ...\n",
       "  6712  2024-07-14 17:00:00+00:00    1.833576e+07\n",
       "  6713  2024-07-14 18:00:00+00:00    1.928075e+07\n",
       "  6714  2024-07-14 19:00:00+00:00    1.830843e+07\n",
       "  6715  2024-07-14 20:00:00+00:00    2.149739e+07\n",
       "  6716  2024-07-14 21:00:00+00:00    2.754799e+07\n",
       "  \n",
       "  [6717 rows x 2 columns],\n",
       "  '20.0%':                            time  throughput_bps\n",
       "  0     2023-10-09 00:00:00+00:00    3.475473e+07\n",
       "  1     2023-10-09 01:00:00+00:00    2.462564e+07\n",
       "  2     2023-10-09 02:00:00+00:00    3.520459e+07\n",
       "  3     2023-10-09 03:00:00+00:00    1.839942e+07\n",
       "  4     2023-10-09 04:00:00+00:00             NaN\n",
       "  ...                         ...             ...\n",
       "  6712  2024-07-14 17:00:00+00:00    1.833576e+07\n",
       "  6713  2024-07-14 18:00:00+00:00    1.928075e+07\n",
       "  6714  2024-07-14 19:00:00+00:00    1.830843e+07\n",
       "  6715  2024-07-14 20:00:00+00:00    2.149739e+07\n",
       "  6716  2024-07-14 21:00:00+00:00    2.754799e+07\n",
       "  \n",
       "  [6717 rows x 2 columns],\n",
       "  '30.0%':                            time  throughput_bps\n",
       "  0     2023-10-09 00:00:00+00:00    3.475473e+07\n",
       "  1     2023-10-09 01:00:00+00:00    2.462564e+07\n",
       "  2     2023-10-09 02:00:00+00:00    3.520459e+07\n",
       "  3     2023-10-09 03:00:00+00:00    1.839942e+07\n",
       "  4     2023-10-09 04:00:00+00:00             NaN\n",
       "  ...                         ...             ...\n",
       "  6712  2024-07-14 17:00:00+00:00    1.833576e+07\n",
       "  6713  2024-07-14 18:00:00+00:00    1.928075e+07\n",
       "  6714  2024-07-14 19:00:00+00:00    1.830843e+07\n",
       "  6715  2024-07-14 20:00:00+00:00    2.149739e+07\n",
       "  6716  2024-07-14 21:00:00+00:00    2.754799e+07\n",
       "  \n",
       "  [6717 rows x 2 columns],\n",
       "  '40.0%':                            time  throughput_bps\n",
       "  0     2023-10-09 00:00:00+00:00    3.475473e+07\n",
       "  1     2023-10-09 01:00:00+00:00    2.462564e+07\n",
       "  2     2023-10-09 02:00:00+00:00    3.520459e+07\n",
       "  3     2023-10-09 03:00:00+00:00    1.839942e+07\n",
       "  4     2023-10-09 04:00:00+00:00             NaN\n",
       "  ...                         ...             ...\n",
       "  6712  2024-07-14 17:00:00+00:00             NaN\n",
       "  6713  2024-07-14 18:00:00+00:00    1.928075e+07\n",
       "  6714  2024-07-14 19:00:00+00:00    1.830843e+07\n",
       "  6715  2024-07-14 20:00:00+00:00    2.149739e+07\n",
       "  6716  2024-07-14 21:00:00+00:00    2.754799e+07\n",
       "  \n",
       "  [6717 rows x 2 columns]}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def introduce_missing_data(df, missing_rate, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df_missing = df.copy()\n",
    "    mask = rng.random(len(df_missing)) < missing_rate\n",
    "    df_missing.loc[mask, \"throughput_bps\"] = np.nan\n",
    "    return df_missing\n",
    "\n",
    "datasets_missing = {}\n",
    "\n",
    "for file in os.listdir(BASE_DIR):\n",
    "    # print(f\"Lista de diretórios: {os.listdir(BASE_DIR)}\")\n",
    "    dataset_path = os.path.join(BASE_DIR, file)\n",
    "    if file.endswith(\".csv\"):\n",
    "        datasets_missing[file] = {} \n",
    "        df = pd.read_csv(dataset_path)\n",
    "        for rate in [0.1, 0.2, 0.3, 0.4]:\n",
    "            df_missing = introduce_missing_data(df, missing_rate=rate, seed=42)\n",
    "            # print(f\"df_missing: {df_missing.head()}\")\n",
    "            key = f\"{file.removesuffix(\".throughput.csv\")}_missing_{int(rate*100)}\"\n",
    "            # print(f\"Lista de keys: {key}\")\n",
    "            datasets_missing[file][f\"{rate*100}%\"] = df_missing\n",
    "\n",
    "datasets_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e783449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_datetime_index(df):\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        try:\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "        except Exception:\n",
    "            print(\"WARNING: It was not possible to ensure datetime index.\")\n",
    "            first_col = df.columns[0]\n",
    "            df[first_col] = pd.to_datetime(df[first_col], errors=\"coerce\")\n",
    "            df = df.set_index(first_col)\n",
    "    return df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7c5c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_seasonal_decomposition(series, period=24):\n",
    "    try:\n",
    "        filled = series.interpolate(method=\"time\", limit_direction=\"both\")\n",
    "        if not isinstance(series.index, pd.DatetimeIndex) or len(filled) < 3 * period:\n",
    "            rolling_mean = filled.rolling(period, min_periods=1).mean()\n",
    "            if isinstance(series.index, pd.DatetimeIndex):\n",
    "                seasonal_pattern = filled.groupby(filled.index.hour).transform(\"median\")\n",
    "            else:\n",
    "                seasonal_pattern = pd.Series(0, index=filled.index)\n",
    "            return rolling_mean, seasonal_pattern - seasonal_pattern.mean(), filled - rolling_mean - seasonal_pattern\n",
    "        \n",
    "        decomp = seasonal_decompose(filled, model=\"additive\", period=period, extrapolate_trend=\"freq\")\n",
    "        return decomp.trend, decomp.seasonal, decomp.resid\n",
    "    except Exception:\n",
    "        return None, None, None\n",
    "\n",
    "def create_overlapping_matrix(series, window_size=72, overlap=0.8):\n",
    "    \"\"\"Cria matriz temporal com sobreposição otimizada para dados de rede\"\"\"\n",
    "    vals = series.values.astype(np.float64)\n",
    "    n = len(vals)\n",
    "    step = max(1, int(window_size * (1 - overlap)))\n",
    "    windows = [vals[i:i+window_size] for i in range(0, n - window_size + 1, step)]\n",
    "    return np.array(windows).T\n",
    "\n",
    "def reconstruct_series(matrix, orig_index):\n",
    "    if matrix.shape[1] == 1:\n",
    "        return pd.Series(matrix[:, 0], index=orig_index[:len(matrix)])\n",
    "    \n",
    "    reconstructed = np.zeros(len(orig_index))\n",
    "    weights = np.zeros(len(orig_index))\n",
    "    \n",
    "    for i in range(matrix.shape[1]):\n",
    "        start = i\n",
    "        for j in range(matrix.shape[0]):\n",
    "            idx = start + j\n",
    "            if idx < len(orig_index) and not np.isnan(matrix[j, i]):\n",
    "                reconstructed[idx] += matrix[j, i]\n",
    "                weights[idx] += 1\n",
    "    \n",
    "    return pd.Series(np.where(weights > 0, reconstructed/weights, np.nan), \n",
    "                    index=orig_index)\n",
    "\n",
    "def evaluate_imputation(df_missing, imputed, df_true):\n",
    "    missing_mask = df_missing[\"throughput\"].isna()\n",
    "    valid_mask = missing_mask & imputed.notna()\n",
    "    if valid_mask.sum() == 0:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    y_true = df_true.loc[valid_mask, \"throughput\"]\n",
    "    y_pred = imputed.loc[valid_mask]\n",
    "    return (np.sqrt(mean_squared_error(y_true, y_pred)), \n",
    "            np.mean(np.abs(y_true - y_pred)))\n",
    "\n",
    "def normalize_series(series):\n",
    "    \"\"\"Normalização robusta para dados de rede com distribuição assimétrica\"\"\"\n",
    "    log_vals = np.log1p(np.abs(series)) * np.sign(series)\n",
    "    median = np.nanmedian(log_vals)\n",
    "    mad = np.nanmedian(np.abs(log_vals - median))\n",
    "    return (log_vals - median) / (mad + 1e-8), median, mad\n",
    "\n",
    "def denormalize_series(norm_vals, median, mad):\n",
    "    \"\"\"Desnormalização para dados de rede\"\"\"\n",
    "    log_vals = norm_vals * (mad + 1e-8) + median\n",
    "    return np.sign(log_vals) * (np.expm1(np.abs(log_vals)))\n",
    "\n",
    "def optimize_svd_parameters(matrix):\n",
    "    \"\"\"Seleção automática de parâmetros SVD baseada na estrutura dos dados\"\"\"\n",
    "    n, m = matrix.shape\n",
    "    rank = min(15, max(5, int(min(n, m) * 0.3)))\n",
    "    shrinkage = max(0.1, min(1.0, np.count_nonzero(np.isnan(matrix)) / (n * m)))\n",
    "    return {'rank': rank, 'shrinkage': shrinkage}\n",
    "\n",
    "# Funções principais otimizadas\n",
    "def knn_imputer(df, n_neighbors=5):\n",
    "    features = []\n",
    "    \n",
    "    # Série original\n",
    "    features.append(df[\"throughput\"])\n",
    "    \n",
    "    # Features temporais\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        features.append(pd.Series(df.index.hour, index=df.index))\n",
    "        features.append(pd.Series(np.sin(2 * np.pi * df.index.hour / 24), index=df.index))\n",
    "        features.append(pd.Series(np.cos(2 * np.pi * df.index.hour / 24), index=df.index))\n",
    "    \n",
    "    # Lags\n",
    "    for i in range(1, 6):\n",
    "        features.append(df[\"throughput\"].shift(i))\n",
    "    \n",
    "    # Médias móveis\n",
    "    features.append(df[\"throughput\"].rolling(24, min_periods=1).mean())\n",
    "    \n",
    "    feature_matrix = pd.concat(features, axis=1).fillna(0)\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    imputed_values = imputer.fit_transform(feature_matrix)\n",
    "    return pd.Series(imputed_values[:, 0], index=df.index)\n",
    "\n",
    "def hybrid_imputation(df):\n",
    "    trend, seasonal, resid = robust_seasonal_decomposition(df[\"throughput\"])\n",
    "    if trend is None:\n",
    "        return df[\"throughput\"].interpolate()\n",
    "    \n",
    "    window_size = min(72, len(df))\n",
    "    resid_matrix = create_overlapping_matrix(resid.fillna(0), window_size)\n",
    "    resid_imputed = SoftImpute().fit_transform(resid_matrix)\n",
    "    resid_series = reconstruct_series(resid_imputed, df.index).fillna(0)\n",
    "    return (trend.fillna(0) + seasonal.fillna(0) + resid_series)\n",
    "\n",
    "def iterative_svd_imputation(df):\n",
    "    window_size = min(72, len(df))\n",
    "    matrix = create_overlapping_matrix(df[\"throughput\"], window_size)\n",
    "    rank = min(15, matrix.shape[1]//2, matrix.shape[0]//2)\n",
    "    imputed = IterativeSVD(rank=rank).fit_transform(matrix)\n",
    "    return reconstruct_series(imputed, df.index)\n",
    "\n",
    "\n",
    "def weighted_reconstruction(matrix, series_index):\n",
    "    \"\"\"Reconstrução ponderada com pesos temporais\"\"\"\n",
    "    n, m = matrix.shape\n",
    "    step = max(1, int(n * 0.2))\n",
    "    reconstructed = np.zeros(len(series_index))\n",
    "    weights = np.zeros(len(series_index))\n",
    "    \n",
    "    # Pesos triangulares (mais peso no centro da janela)\n",
    "    window_weights = 1.0 - np.abs(np.linspace(-1, 1, n))\n",
    "    \n",
    "    for i in range(m):\n",
    "        start_idx = i * step\n",
    "        end_idx = start_idx + n\n",
    "        \n",
    "        for j in range(n):\n",
    "            idx = start_idx + j\n",
    "            if idx < len(series_index) and not np.isnan(matrix[j, i]):\n",
    "                weight = window_weights[j]\n",
    "                reconstructed[idx] += matrix[j, i] * weight\n",
    "                weights[idx] += weight\n",
    "    \n",
    "    # Normalização e tratamento de bordas\n",
    "    reconstructed = np.where(weights > 0, reconstructed / weights, np.nan)\n",
    "    return pd.Series(reconstructed, index=series_index)\n",
    "\n",
    "def enhanced_hybrid_imputation(df):\n",
    "    \"\"\"Imputação híbrida aprimorada para dados de rede\"\"\"\n",
    "    # Passo 1: Normalização robusta\n",
    "    norm_vals, median, mad = normalize_series(df['throughput'])\n",
    "    \n",
    "    # Passo 2: Decomposição sazonal\n",
    "    trend, seasonal, resid = robust_seasonal_decomposition(\n",
    "        pd.Series(norm_vals, index=df.index)\n",
    "    )\n",
    "    \n",
    "    # Passo 3: Criação da matriz de resíduos\n",
    "    resid_matrix = create_overlapping_matrix(resid.fillna(0))\n",
    "    \n",
    "    # Passo 4: Otimização e aplicação do SVD\n",
    "    params = optimize_svd_parameters(resid_matrix)\n",
    "    if params['shrinkage'] > 0:\n",
    "        svd_imputer = SoftImpute(shrinkage_value=params['shrinkage'])\n",
    "    else:\n",
    "        svd_imputer = IterativeSVD(rank=params['rank'])\n",
    "    \n",
    "    imputed_resid = svd_imputer.fit_transform(resid_matrix)\n",
    "    \n",
    "    # Passo 5: Reconstrução ponderada\n",
    "    resid_series = weighted_reconstruction(imputed_resid, df.index)\n",
    "    \n",
    "    # Passo 6: Combinação de componentes\n",
    "    combined = trend.fillna(0) + seasonal.fillna(0) + resid_series.fillna(0)\n",
    "    \n",
    "    # Passo 7: Desnormalização\n",
    "    return denormalize_series(combined, median, mad)\n",
    "\n",
    "# Atualização do pipeline principal\n",
    "def main_pipeline(data_folder):\n",
    "    datasets = {}\n",
    "    for file in os.listdir(data_folder):\n",
    "        if file.endswith(\".csv\"):\n",
    "            try:\n",
    "                df = pd.read_csv(os.path.join(data_folder, file))\n",
    "                time_col, thr_col = df.columns[0], df.columns[1]\n",
    "                df = df.set_index(pd.to_datetime(df[time_col])).rename(columns={thr_col: \"throughput\"})[[\"throughput\"]]\n",
    "                if not df[\"throughput\"].dropna().empty:\n",
    "                    datasets[file] = df\n",
    "            except Exception:\n",
    "                continue\n",
    "    \n",
    "    if not datasets:\n",
    "        return None\n",
    "\n",
    "    results = []\n",
    "    missing_rates = [0.1, 0.2, 0.3, 0.4]\n",
    "    \n",
    "    for name, df_true in datasets.items():\n",
    "        for rate in missing_rates:\n",
    "            df_missing = introduce_missing_data(df_true.copy(), rate)\n",
    "            key = f\"{name}_missing_{int(rate*100)}%\"\n",
    "            \n",
    "            # Métodos básicos\n",
    "            median_imp = df_missing[\"throughput\"].fillna(df_missing[\"throughput\"].median())\n",
    "            interp_imp = df_missing[\"throughput\"].interpolate(method='time').fillna(median_imp)\n",
    "            \n",
    "            # Métodos avançados\n",
    "            try:\n",
    "                hybrid_imp = enhanced_hybrid_imputation(df_missing)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro no método híbrido: {e}\")\n",
    "                hybrid_imp = interp_imp.copy()\n",
    "            \n",
    "            # Avaliação\n",
    "            metrics = {\n",
    "                \"dataset\": key,\n",
    "                \"missing_rate\": rate,\n",
    "                \"median_rmse\": evaluate_imputation(df_missing, median_imp, df_true)[0],\n",
    "                \"interpolation_rmse\": evaluate_imputation(df_missing, interp_imp, df_true)[0],\n",
    "                \"hybrid_rmse\": evaluate_imputation(df_missing, hybrid_imp, df_true)[0]\n",
    "            }\n",
    "            results.append(metrics)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "# Execução\n",
    "if __name__ == \"__main__\":\n",
    "    results_df = main_pipeline(\"data/\")\n",
    "    if results_df is not None:\n",
    "        print(results_df)\n",
    "        results_df.to_csv(\"results.csv\", index=False)\n",
    "    else:\n",
    "        print(\"No valid datasets found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
